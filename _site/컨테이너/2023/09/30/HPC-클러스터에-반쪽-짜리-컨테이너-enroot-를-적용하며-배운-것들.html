<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="bdg-blog" /></head>
<style>@import url(/public/css/syntax/monokai.css);</style>
  <title>bdg-blog</title>
  <!-- <link href="/public/css/bootstrap.min.css" rel="stylesheet"> -->

  <link href="/public/css/style.css" rel="stylesheet">
  <body>
  	<div class="container"> 
		<div class="sidebar">
			<div class="sidebar-item sidebar-header">
	<div class='sidebar-brand'>
		<a href="/">bdg-blog</a>
	</div>
	<p class="lead"></p></div>

<div class="sidebar-item sidebar-nav">
	<ul class="nav">
      <li class="nav-title">Pages</li>
	  <li>
	  	<a class="nav-item" href="/">Articles</a>
	  </li>
	  
	  
	    
	  
	    
	      
	    
	  
	    
	  
	    
	  
	</ul>
</div>

<div class="sidebar-item sidebar-nav">
  	<ul class="nav">
			<li class="nav-title">Categories</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#쿠버네티스">
				<span class="name">쿠버네티스</span>
				<span class="badge">10</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#기타">
				<span class="name">기타</span>
				<span class="badge">8</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#운영체제/컴퓨터구조">
				<span class="name">운영체제/컴퓨터구조</span>
				<span class="badge">7</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#백앤드">
				<span class="name">백앤드</span>
				<span class="badge">5</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#컨테이너">
				<span class="name">컨테이너</span>
				<span class="badge">2</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Go">
				<span class="name">Go</span>
				<span class="badge">6</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#클린 아키텍쳐">
				<span class="name">클린 아키텍쳐</span>
				<span class="badge">2</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#보안">
				<span class="name">보안</span>
				<span class="badge">2</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#리눅스">
				<span class="name">리눅스</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#네트워크">
				<span class="name">네트워크</span>
				<span class="badge">4</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#자바스크립트">
				<span class="name">자바스크립트</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#클린 코드">
				<span class="name">클린 코드</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	  </nav>
	</ul>
</div>

<div class="sidebar-item sidebar-footer">
</div>
		</div>
		<div class="content">
			<article class="post">
	<header class="post-header">
		<div class="post-title"> 
			HPC 클러스터에 반쪽 짜리 컨테이너 enroot 를 적용하며 배운 것들
		</div>
		<time class="post-date dt-published" datetime="2023-09-30T23:59:46+09:00" itemprop="datePublished">2023/09/30
		</time>		
	</header>

	<div class="post-content">
		<h1 id="배경">배경</h1>

<p>현재 관리하고 있는 HPC 클러스터는 100대의 연산 노드가 Centos7을 사용하고 있습니다. 클러스터의 사용자는 접속이 가능한 gate노드에 ssh로 접속하여 slurm 명령어를 통해서 작업을 제출합니다. slurm은 작업 스케줄링 도구로, 사용자가 제출한 작업을 적절한 연산 노드에 할당하여 작업을 수행합니다. slurm은 cgroup을 사용하여 사용자가 실행한 프로세스의 자원(cpu, memory, gpu)를 제한할 수 있습니다. 또한 사용자는 클러스터에 설치된 Linux Environment Modules 중 필요한 환경을 load 하여 사용합니다. 만약 여기에 필요한 환경이 없다면 권한이 있는 경로에 컴파일된 바이너리 실행파일 혹은 파이썬 가상환경을 직접 설치하여 사용하고 있습니다.</p>

<p><img alt="image" src="/images/7bc5601f-95a1-4008-bb77-92ad2da061b4" /></p>

<h1 id="문제점">문제점</h1>

<p>Linux Environment Modules는 사용자가 여러 버전의 애플리케이션, 라이브러리, 그리고 기타 소프트웨어 패키지를 쉽게 로드하거나 언로드할 수 있도록 도와주는 툴입니다. 국내에 많은 연구기관에서 사용되고 있지만, 다양한 문제점을 가지고 있습니다.</p>

<h3 id="운영체제-의존">운영체제 의존</h3>

<p>Linux Environment Modules는 Host 운영체제에 의존하여 동작합니다. 사용자가 Ubuntu와 같은 다른 운영체제의 패키지를 사용하고자 한다면 사용에 제한이 있습니다.</p>

<h3 id="불완전한-격리"><strong>불완전한 격리</strong></h3>

<p>Linux Environment Modules는 완전한 환경 격리를 제공하지 않습니다. 따라서 서로 다른 의존성 라이브러리가 시스템에 설치되어 있을 때 문제가 발생할 수 있습니다.</p>

<h3 id="충돌"><strong>충돌</strong></h3>

<p>불완전한 격리의 연장선으로 충돌 문제가 있습니다. 사용자가 여러 모듈을 동시에 로드할 때, 라이브러리나 의존성 충돌이 발생할 수 있습니다.</p>

<h3 id="관리-포인트-증가">관리 포인트 증가</h3>

<p>Linux Environment Modules의 환경은 관리자만 생성할 수 있습니다. 사용자의 요청할 때마다 관리자는 새로운 환경을 빌드하고 등록해야 합니다.</p>

<h2 id="해결방법">해결방법</h2>

<p>처음 생각한 방법은 Docker와 같은 컨테이너를 도입하는 것입니다. 혹은 클러스터에 쿠버네티스를 설치한다면 사용자가 원하는 이미지를 사용해 완전히 환경을 격리할 수 있다고 생각했습니다. 하지만 여기에는 몇 가지 문제가 있었습니다. 우선 기존의 Slurm + Linux Environment Modules 사용자를 고려해야 한다는 점입니다. 기존의 사용자는 몇 년간 위 환경으로 연구를 진행해 왔고, 사용하던 환경을 그대로 쓰고 싶어 했습니다.</p>

<p>따라서 기존의 Slurm과 호환되는 커테이너를 사용해야 합니다. 하지만, Docker 및 K8s는 슬럼과 호환되지 않습니다. Slurm은 자체적으로 cgroup을 사용해 job 별 자원을 제한합니다. 하지만 이러한 기능이 K8s나 Docker와 충돌하기 때문에 Slurm과 함께 사용할 수 없었습니다.</p>

<p>여기서 찾은 해결책은 Slurm을 그대로 사용하면서 NVIDIA 사에서 개발한 Enroot라는 반쪽 짜리 컨테이너를 도입하는 것입니다. Enroot는 HCP 클러스터에서 컨테이너와 같은 방식으로 환경을 분리하기 위해 만들어진 프로그램입니다. CNI를 따르지 않으며 자원을 제약하는 기능이 없습니다. 단순히 linux namespace를 통해 환경을 격리하는 기능만 제공합니다. 하지만 docker image를 enroot image로 변환하는 기능을 제공하기 때문에, 사용자가 손쉽게 컨테이너 이미지를 만들 수 있고, slurm에서 enroot 관련 플러그인을 제공하고 있습니다. 또한 NVIDIA 사에서 개발한 만큼, NVIDIA GPU 사용 기능을 기본적으로 제공합니다. enroot를 사용하면 현재 클러스터의 설정을 크게 바꾸지 않고 컨테이너 환경을 구축할 수 있다고 생각해 enroot를 사용하기로 결정했습니다.</p>

<p><img alt="image" src="/images/ff2206c1-259b-4038-87f2-807d5f9ad2bc" /></p>

<ul>
  <li>https://github.com/NVIDIA/enroot</li>
</ul>

<h1 id="설치-방법">설치 방법</h1>

<h2 id="커널-커맨드-및-커널-파라미터-수정">커널 커맨드 및 커널 파라미터 수정</h2>

<p>enroot 를 설치하기 위해서는 커널 커맨드 및 커널 파라미터를 수정해야 합니다. 아래 문서를 참고하면서 하나씩 진행하겠습니다.</p>

<ul>
  <li>https://github.com/NVIDIA/enroot/blob/master/doc/requirements.md
    <ul>
      <li><code class="highlighter-rouge">./enroot-check_*.run --verify</code></li>
    </ul>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="o">[</span>root@gate1 container]# ./enroot-check_<span class="k">*</span>.run <span class="nt">--verify</span> 
  Kernel version:
    
  Linux version 3.10.0-1127.el7.x86_64 <span class="o">(</span>root@tgm-master.hpc<span class="o">)</span> <span class="o">(</span>gcc version 4.8.5 20150623 <span class="o">(</span>Red Hat 4.8.5-39<span class="o">)</span> <span class="o">(</span>GCC<span class="o">)</span> <span class="o">)</span> <span class="c">#1 SMP Thu May 7 14:42:14 KST 2020</span>
    
  Kernel configuration:
    
  CONFIG_NAMESPACES                 : OK
  CONFIG_USER_NS                    : OK
  CONFIG_SECCOMP_FILTER             : OK
  CONFIG_OVERLAY_FS                 : OK <span class="o">(</span>module<span class="o">)</span>
  CONFIG_X86_VSYSCALL_EMULATION     : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
  CONFIG_VSYSCALL_EMULATE           : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
  CONFIG_VSYSCALL_NATIVE            : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
    
  Kernel <span class="nb">command </span>line:
    
  <span class="k">**</span>namespace.unpriv_enable<span class="o">=</span>1         : KO
  user_namespace.enable<span class="o">=</span>1           : KO<span class="k">**</span>
  <span class="nv">vsyscall</span><span class="o">=</span>native                   : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
  <span class="nv">vsyscall</span><span class="o">=</span>emulate                  : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
    
  Kernel parameters:
    
  <span class="k">**</span>user.max_user_namespaces          : KO<span class="k">**</span>
  user.max_mnt_namespaces           : OK
    
  Extra packages:
    
  <span class="k">**</span>nvidia-container-cli              : KO<span class="k">**</span> 
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="grubby를-사용한-kernel-command-line-수정">grubby를 사용한 kernel command line 수정</h3>

<p>노드의 부팅이미지가 디스크에 있는 경우 grubby 를 이용해서 kernel의 실행 커맨드를 쉽게 수정할 수 있습니다. enroot를 설치한 클러스터에서 게이트 노드, 관리 노드는 아래와 같이 부팅 설정을 변경하고 시스템을 재부팅하여 손쉽게 적용했습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>grubby <span class="nt">--args</span><span class="o">=</span><span class="s2">"namespace.unpriv_enable=1 user_namespace.enable=1"</span> <span class="nt">--update-kernel</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>grubby <span class="nt">--default-kernel</span><span class="si">)</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s1">'user.max_user_namespaces=15076'</span> <span class="o">&gt;&gt;</span> /etc/sysctl.conf

sysctl <span class="nt">--system</span>
</code></pre></div></div>

<h3 id="pxelinuxcfg-파일을-통한-kernel-command-line-수정">pxelinux.cfg 파일을 통한 kernel command line 수정</h3>

<p>하지만, 계산 노드는 pxe boot 라는 방식을 이용합니다. pxe(preboot execution environment) boot란 네트워크를 통해 컴퓨터를 부팅하는 기술입니다. pxe server에 저장된 커널 이미지를 pxe client 노드들이 네트워크를 통해서 다운받고, 이를 부팅하는 방식으로 동작합니다. 마스터 노드가 pxe 서버 역할을 하기 때문에, 만약 계산 노드들의 부팅 이미지, 혹은 설정을 변경하기 위해서는 마스터 노드의 pxe 관련 설정을 수정해야 합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/RBOOT/efi64/pxelinux.cfg
</code></pre></div></div>

<p>마스터 노드의 위 경로에 노드별로 부팅 설정 파일이 저장되어 있었습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="o">[</span>root@master pxelinux.cfg]# <span class="nb">ls
</span>C0A86401  C0A86412  C0A86423  C0A86434  C0A86445  C0A86456  C0A86467  C0A86478  C0A86489
C0A86402  C0A86413  C0A86424  C0A86435  C0A86446  C0A86457  C0A86468  C0A86479  C0A8648A
C0A86403  C0A86414  C0A86425  C0A86436  C0A86447  C0A86458  C0A86469  C0A8647A  C0A8648B
C0A86404  C0A86415  C0A86426  C0A86437  C0A86448  C0A86459  C0A8646A  C0A8647B  C0A8648C
C0A86405  C0A86416  C0A86427  C0A86438  C0A86449  C0A8645A  C0A8646B  C0A8647C  C0A8648D
...

<span class="o">[</span>root@master pxelinux.cfg]# <span class="nb">cat </span>C0A86401
DEFAULT TGMkernel 

LABEL TGMkernel
   KERNEL http://192.168.100.1/KERNEL/kernel.img
   APPEND <span class="nv">initrd</span><span class="o">=</span>http://192.168.100.1/KERNEL/n033_ramfs.tgm quiet 
		rd_NO_DM <span class="nv">ip</span><span class="o">=</span>eth0:dhcp <span class="nv">ETHERNET</span><span class="o">=</span>eth0 <span class="nv">NISDOMAIN</span><span class="o">=</span>TGM rw <span class="nv">selinux</span><span class="o">=</span>0 
		net.ifnames<span class="o">=</span>0 <span class="nv">biosdevname</span><span class="o">=</span>0 intel_idle.max_cstate<span class="o">=</span>0 processor.max_cstate<span class="o">=</span>1 
		ipv6.disable<span class="o">=</span>1 <span class="nv">cgroup_enable</span><span class="o">=</span>memory <span class="nv">swapaccount</span><span class="o">=</span>1 <span class="nv">intel_pstate</span><span class="o">=</span>disable <span class="nv">rdblacklist</span><span class="o">=</span>nouveau 
		nouveau.modeset<span class="o">=</span>0 <span class="nv">vga</span><span class="o">=</span>normal nofb i915.modeset<span class="o">=</span>0 <span class="nv">spectre_v2</span><span class="o">=</span>off nospectre_v1 nospectre_v2 
		nopti rhgb nomodeset <span class="nv">video</span><span class="o">=</span>vesafb:off <span class="nv">panic</span><span class="o">=</span>60 
		nfs.nfs4_unique_id<span class="o">=</span>3dc56ea0-d930-464c-80e6-1ec1f9f4b1c1  
</code></pre></div></div>

<p>해당 파일마다  <strong><code class="highlighter-rouge">namespace.unpriv_enable=1 user_namespace.enable=1</code></strong> 라인을 추가하여 pxe 설정파일을 수정했습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nb">cd</span> /path/to/RBOOT/efi64/

<span class="nb">cp</span> <span class="nt">-pr</span> pxelinux.cfg pxelinux.cfg.bk

<span class="nv">list</span><span class="o">=</span><span class="sb">`</span><span class="nb">ls </span>pxelinux.cfg<span class="sb">`</span>

<span class="k">for </span>text <span class="k">in</span> <span class="nv">$list</span>
<span class="k">do
    </span><span class="nb">echo</span> <span class="nv">$text</span>
    <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'$s/$/namespace.unpriv_enable=1 user_namespace.enable=1/'</span> <span class="nt">-s</span> <span class="nv">$text</span> <span class="o">&gt;</span> <span class="nv">$text</span>
<span class="k">done</span>
</code></pre></div></div>

<p>그 결과, 모든 계산 노드의 커널 커맨드가 수정되었습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@tgm-master pxelinux.cfg]# <span class="nb">cat </span>C0A86401
DEFAULT TGMkernel 

LABEL TGMkernel
   KERNEL http://pxe-server-ip/KERNEL/kernel.img
   APPEND <span class="nv">initrd</span><span class="o">=</span>http://pxe-server-ip/KERNEL/n0XX_ramfs.tgm quiet 
		rd_NO_DM <span class="nv">ip</span><span class="o">=</span>eth0:dhcp <span class="nv">ETHERNET</span><span class="o">=</span>eth0 <span class="nv">NISDOMAIN</span><span class="o">=</span>TGM rw <span class="nv">selinux</span><span class="o">=</span>0 
		net.ifnames<span class="o">=</span>0 <span class="nv">biosdevname</span><span class="o">=</span>0 intel_idle.max_cstate<span class="o">=</span>0 processor.max_cstate<span class="o">=</span>1 
		ipv6.disable<span class="o">=</span>1 <span class="nv">cgroup_enable</span><span class="o">=</span>memory <span class="nv">swapaccount</span><span class="o">=</span>1 <span class="nv">intel_pstate</span><span class="o">=</span>disable <span class="nv">rdblacklist</span><span class="o">=</span>nouveau 
		nouveau.modeset<span class="o">=</span>0 <span class="nv">vga</span><span class="o">=</span>normal nofb i915.modeset<span class="o">=</span>0 <span class="nv">spectre_v2</span><span class="o">=</span>off nospectre_v1 nospectre_v2 
		nopti rhgb nomodeset <span class="nv">video</span><span class="o">=</span>vesafb:off <span class="nv">panic</span><span class="o">=</span>60 
		nfs.nfs4_unique_id<span class="o">=</span>3dc56ea0-d930-464c-80e6-1ec1f9f4b1c1  
		<span class="k">**</span>namespace.unpriv_enable<span class="o">=</span>1 user_namespace.enable<span class="o">=</span>1<span class="k">**</span>
</code></pre></div></div>

<h3 id="kernel-parameters-수정">Kernel parameters 수정</h3>

<p>커널 파라미터는 <code class="highlighter-rouge">/etc/sysctl.conf</code> 를 수정하여 변경할 수 있는데, 모든 계산 노드 etc 폴더가 마스터 노드에 NFS로 마운트되어 있습니다. 따라서 마스터 노드에서 일괄적으로 수정이 가능합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@tgm-master NODES]#
<span class="nv">list</span><span class="o">=</span><span class="sb">`</span><span class="nb">ls</span> ./<span class="k">*</span>/etc/sysctl.conf<span class="sb">`</span>

<span class="k">for </span>loc <span class="k">in</span> <span class="nv">$list</span>
<span class="k">do
    </span><span class="nb">echo</span> <span class="s1">'user.max_user_namespaces=15076'</span> <span class="o">&gt;&gt;</span> <span class="nv">$loc</span>
<span class="k">done</span>
</code></pre></div></div>

<p>이후 모든 계산노드를 재부팅하여 설정을 갱신했습니다.</p>

<h2 id="nvidia-container-cli-설치">nvidia-container-cli 설치</h2>

<p>enroot로 gpu를 사용하기 위해서는 nvidia-container-cli이 필요합니다. 아래 가이드를 참고해 설치했습니다.</p>

<ul>
  <li>https://github.com/NVIDIA/libnvidia-container</li>
  <li>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># yum 레파지토리 추가</span>
<span class="nv">distribution</span><span class="o">=</span><span class="si">$(</span><span class="nb">.</span> /etc/os-release<span class="p">;</span><span class="nb">echo</span> <span class="nv">$ID$VERSION_ID</span><span class="si">)</span>    <span class="o">&amp;&amp;</span> curl <span class="nt">-s</span> <span class="nt">-L</span> https://nvidia.github.io/libnvidia-container/<span class="nv">$distribution</span>/libnvidia-container.repo | <span class="nb">sudo tee</span> /etc/yum.repos.d/nvidia-container-toolkit.repo
yum-config-manager <span class="nt">--enable</span> libnvidia-container-experimental
yum clean expire-cache
<span class="c"># nvidia-container-cli 설치</span>
yum <span class="nb">install</span> <span class="nt">-y</span> libnvidia-container1
yum <span class="nb">install</span> <span class="nt">-y</span> libnvidia-container-tools
</code></pre></div></div>

<ul>
  <li>
    <p>nvidia_uvm이 없는 문제 해결</p>

    <p>일부 노드에서  NVIDIA driver가 완전하기 실행되지 않고, UVM 커널 모듈이 빠진 경우 아래와 같은 오류가 발생했습니다. <code class="highlighter-rouge">nvidia-container-cli -k info</code> 커맨드를 통해서 드라이버를 다시 로드하여 해결했습니다. 만약 같은 문제가 반복된다면, 시스템 데몬으로 등록하여 리로드를 강제할 수 있다고 합니다.</p>

    <p>(https://github.com/NVIDIA/enroot/issues/105)</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># nvidia_uvm 없음.</span>
<span class="c">#[WARN] Kernel module nvidia_uvm is not loaded. Make sure the NVIDIA device driver is installed and loaded.</span>
<span class="nv">$# </span>nvidia-container-cli <span class="nt">-k</span> info
</code></pre></div></div>

<ul>
  <li>
    <p>libnvidia-ml.so 못 찾는 문제 해결</p>

    <p>찾아보니 debian 계열의 운영체제에서 유사한 에러가 발생했고, 원인은  /sbin/ldconfig 와 관련된 경로가 달라서 생기는 오류라고 합니다.</p>

    <p>이 클러스터는 centos7이기 때문에 완전히 같은 원인은 아니겠지만, 비슷한 이유로 ldconfig를 실행하지 못해 공유 라이브러리 링커가 잘 연결되지 않았다면, *.so 관련 에러가 발생할 수 있을 것이라 생각했습니다. <code class="highlighter-rouge">ldconfig</code>를 수동으로 실행했더니 문제가 해결됐습니다.</p>
  </li>
</ul>

<h2 id="다시-enroot-pre-requirement-검사">다시 Enroot pre-requirement 검사</h2>

<p>필요한 설정들을 마치고 다시 enroot-pre-requirement를 실행하여 시스템 설정이 잘 적용 됐는지 확인했습니다.</p>

<ul>
  <li><code class="highlighter-rouge">./enroot-check_*.run --verify</code></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@gate1 container]# ./enroot-check_<span class="k">*</span>.run <span class="nt">--verify</span> 
Kernel version:

Linux version 3.10.0-1127.el7.x86_64 <span class="o">(</span>root@tgm-master.hpc<span class="o">)</span> <span class="o">(</span>gcc version 4.8.5 20150623 <span class="o">(</span>Red Hat 4.8.5-39<span class="o">)</span> <span class="o">(</span>GCC<span class="o">)</span> <span class="o">)</span> <span class="c">#1 SMP Thu May 7 14:42:14 KST 2020</span>

Kernel configuration:

CONFIG_NAMESPACES                 : OK
CONFIG_USER_NS                    : OK
CONFIG_SECCOMP_FILTER             : OK
CONFIG_OVERLAY_FS                 : OK <span class="o">(</span>module<span class="o">)</span>
CONFIG_X86_VSYSCALL_EMULATION     : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
CONFIG_VSYSCALL_EMULATE           : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
CONFIG_VSYSCALL_NATIVE            : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>

Kernel <span class="nb">command </span>line:

<span class="k">**</span>namespace.unpriv_enable<span class="o">=</span>1         : OK
user_namespace.enable<span class="o">=</span>1           : OK<span class="k">**</span>
<span class="nv">vsyscall</span><span class="o">=</span>native                   : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>
<span class="nv">vsyscall</span><span class="o">=</span>emulate                  : KO <span class="o">(</span>required <span class="k">if </span>glibc &lt;<span class="o">=</span> 2.13<span class="o">)</span>

Kernel parameters:

user.max_user_namespaces          : OK
user.max_mnt_namespaces           : OK

Extra packages:

<span class="k">**</span>nvidia-container-cli              : OK<span class="k">**</span> 
</code></pre></div></div>

<h2 id="enroot-설치----httpsgithubcomnvidiaenroot">Enroot 설치 -  https://github.com/NVIDIA/enroot</h2>

<p><strong>nvidia-container-cli</strong> 마찬가지로 master 노드에 yum으로 간단하게 설치할 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3. enroot 설치<span class="o">(</span>https://github.com/NVIDIA/enroot<span class="o">)</span>
<span class="nb">arch</span><span class="o">=</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span>
yum <span class="nb">install</span> <span class="nt">-y</span> epel-release
yum <span class="nb">install</span> <span class="nt">-y</span> https://github.com/NVIDIA/enroot/releases/download/v3.4.0/enroot-hardened-3.4.0-2.el7.<span class="k">${</span><span class="nv">arch</span><span class="k">}</span>.rpm
yum <span class="nb">install</span> <span class="nt">-y</span> https://github.com/NVIDIA/enroot/releases/download/v3.4.0/enroot-hardened+caps-3.4.0-2.el7.<span class="k">${</span><span class="nv">arch</span><span class="k">}</span>.rpm
</code></pre></div></div>

<p>enroot 를 설치하면 노드마다 <code class="highlighter-rouge">/etc/enroot/enroot.conf</code> 라는 경로에 설정파일이 생성됩니다. 이 설정 파일을 수정하여 enroot를 커스텀할 수 있다고 합니다. 현재 클러스터 상황에 맞게 일부 설정을 수정하였습니다.</p>

<ul>
  <li>
    <p>runtime, data, temp 경로를 /disk 로 수정</p>

    <p>현재 클러스터는 사용자 계정이 NFS 볼륨에 마운트 되어 있습니다. enroot의 runtime, data, temp 경로를 /home/user 아래에 생성하면, 손쉽게 컨테이너를 동기화할 수 있지만, NFS 볼륨은 사용량이 많아 IO 가 매우 느리다는 단점이 있습니다. 특히 패키지 설치와 같은 쓰기 작업 속도가 매우 느리기 때문에 disk 볼륨을 런타임 및 데이터 경로로 사용하기로 결정했다. 둘의 차이를 비교해본 결과 nfs 볼륨을 사용할 때보다 훨씬 빠른 작업 속도를 보였습니다.</p>
  </li>
  <li>
    <p>이미지 압축 옵션 변경</p>

    <p>enroot는 다양한 압축 알고리즘을 지원합니다. 이 클러스터는 스토리지 공간에 여유가 있기 때문에, 컨테이너 이미지가 커도 압축 속도가 빠른 gzip 알고리즘을 사용하도록 변경했습니다. 또한 특정 레이어 사이즈가 스토리지 블록 사이즈 보다 클 경우, 이미지를 export하지 못하는 문제가 있었는데, 이를 해결하기 위해 fragments를 허용하는 옵션을 추가했습니다.</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># /etc/enroot/enroot.conf</span>
<span class="nv">ENROOT_RUNTIME_PATH</span><span class="o">=</span>/disk/enroot/<span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>/runtime
<span class="nv">ENROOT_DATA_PATH</span><span class="o">=</span>/disk/enroot/<span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>/data
<span class="nv">ENROOT_CACHE_PATH</span><span class="o">=</span>/disk/enroot/<span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>/cache
<span class="nv">ENROOT_TEMP_PATH</span><span class="o">=</span>/disk/enroot/<span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>
<span class="nv">ENROOT_SQUASH_OPTIONS</span><span class="o">=</span><span class="s1">'-comp gzip -noD -always-use-fragments'</span>
ENROOT_MAX_PROCESSORS      10
</code></pre></div></div>

<h1 id="결론">결론</h1>

<p>이제 사용자가 자유롭게 도커 컨테이너로부터 작업 환경을 가져올 수 있게 됐습니다. 더욱이 NFS에 의존하지 않으면서 환경을 구축하고, 실행할 수 있기 때문에 작업 속도가 크게 증가했습니다.</p>

<p>저로서는 이번 enroot를 설치하면서 많은 공부가 되었습니다.  HPC 클러스터에 의존하는 설정 덕분에 다양한 문제들을 마주했고, 이 문제들을 해결해 가면서 운영체제, HPC 구성에 대한 이해를 높일 수 있는 시간이었습니다.</p>

<h1 id="reference">Reference</h1>

<ul>
  <li>https://slurm.schedmd.com/SLUG19/NVIDIA_Containers.pdf</li>
  <li>https://slurm.schedmd.com/qos.html</li>
  <li>https://github.com/NVIDIA/enroot</li>
  <li><strong>“Performance Analysis of Container-based Virtualization for High Performance Computing Environments”</strong> by Kozhirbayev, Zhandos, and Richard O. Sinnott.</li>
</ul>


	</div>
	<br/>
	<br/>
	<script src="https://utteranc.es/client.js"
        repo="deagwon97/deagwon97.github.io"
        issue-term="url"
        theme="github-light"
        crossorigin="anonymous"
        async>
	</script>
</article>
		</div>
	</div>
  </body>
</html>